{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks for MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure training flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from cnn.mnist.cnn_files import files as _files\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def init_training_flags():\n",
    "    \n",
    "    tr_flags = type('TrainingFlags', (object,), {})\n",
    "    \n",
    "    tr_flags.batch_size = 64\n",
    "    tr_flags.test_batch_size = 1000\n",
    "    tr_flags.epochs = 10\n",
    "    tr_flags.no_cuda = False\n",
    "    tr_flags.seed = 1\n",
    "    tr_flags.log_interval = 10\n",
    "    tr_flags.weights = _files.model_file('mnist_weights.pth.tar')\n",
    "    tr_flags.lr = 0.01\n",
    "    tr_flags.momentum = 0.5\n",
    "    \n",
    "    # System configuration\n",
    "    tr_flags.num_workers=8\n",
    "    \n",
    "    tr_flags.cuda = False\n",
    "\n",
    "    return tr_flags\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import (datasets, transforms)\n",
    "\n",
    "flags = init_training_flags()\n",
    "\n",
    "torch.manual_seed(flags.seed)\n",
    "if flags.cuda:\n",
    "  torch.cuda.manual_seed(flags.seed)\n",
    "\n",
    "kwargs = {'num_workers': flags.num_workers, 'pin_memory': True} if flags.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  datasets.MNIST(_files.data_dir, train=True, download=True,\n",
    "                 transform=transforms.Compose([\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize((0.1307,), (0.3081,))\n",
    "                 ])),\n",
    "  batch_size=flags.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  datasets.MNIST(_files.data_dir, train=False, transform=transforms.Compose([\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize((0.1307,), (0.3081,))\n",
    "                 ])),\n",
    "  batch_size=flags.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.models.layers import Flatten\n",
    "\n",
    "\n",
    "class LeNetClassic(nn.Module):\n",
    "  \"\"\"Network model without flatten layer\n",
    "   for character recognition\"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(LeNetClassic, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "    self.conv2_drop = nn.Dropout2d()\n",
    "    self.fc1 = nn.Linear(320, 50)\n",
    "    self.fc2 = nn.Linear(50, 10)\n",
    "  \n",
    "  def forward(self, x):\n",
    "      \n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "    x = x.view(x.size(0), 320)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc2(x)\n",
    "    result = F.log_softmax(x)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "  \"\"\"Network model with flatten layer\n",
    "   for character recognition\"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(LeNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "    self.conv2_drop = nn.Dropout2d()\n",
    "    self.flatten = Flatten(50)\n",
    "    self.fc2 = nn.Linear(50, 10)\n",
    "  \n",
    "  def forward(self, x):\n",
    "      \n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "    x = self.flatten(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc2(x)\n",
    "    result = F.log_softmax(x)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "class LeNetSequential(nn.Module):\n",
    "  \"\"\"Network model with flatten layer\n",
    "   for character recognition\"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(LeNetSequential, self).__init__()\n",
    "    self.conv_part = nn.Sequential(nn.Conv2d(1, 10, kernel_size=5),\n",
    "                                   nn.MaxPool2d(2, 2),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(10, 20, kernel_size=5),\n",
    "                                   nn.MaxPool2d(2, 2),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Dropout2d())\n",
    "    self.flatten = Flatten(50)\n",
    "    self.fc2 = nn.Linear(50, 10)\n",
    "  \n",
    "  def forward(self, x):\n",
    "      \n",
    "    x = self.conv_part(x)\n",
    "    x = self.flatten(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc2(x)\n",
    "    result = F.log_softmax(x)\n",
    "    \n",
    "    return result\n",
    "  \n",
    "  \n",
    "class LeNetSequentialDict(nn.Module):\n",
    "  \"\"\"Network model with flatten layer\n",
    "   for character recognition\"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(LeNetSequentialDict, self).__init__()\n",
    "    self.conv_part = nn.Sequential(OrderedDict([\n",
    "                                   ('conv1', nn.Conv2d(1, 10, kernel_size=5)),\n",
    "                                   ('mxpl1', nn.MaxPool2d(2, 2)),\n",
    "                                   ('relu1', nn.ReLU()),\n",
    "                                   ('conv2', nn.Conv2d(10, 20, kernel_size=5)),\n",
    "                                   ('mxol2', nn.MaxPool2d(2, 2)),\n",
    "                                   ('relu2', nn.ReLU()),\n",
    "                                   ('drop1', nn.Dropout2d())]))\n",
    "    self.flatten = Flatten(50)\n",
    "    self.fc2 = nn.Linear(50, 10)\n",
    "  \n",
    "  def forward(self, x):\n",
    "      \n",
    "    x = self.conv_part(x)\n",
    "    x = self.flatten(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc2(x)\n",
    "    result = F.log_softmax(x)\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End=attachment\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.322252\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.301306\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.324885\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.273060\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.305168\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.296707\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.254200\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.295718\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.282023\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.264656\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.269118\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.296881\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.234677\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.258199\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.192923\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.204647\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.204917\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.210928\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.150971\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.135327\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.184875\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.167610\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.139104\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.193440\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.081496\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.129212\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.995320\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.126444\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.998492\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.916847\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.931619\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.899894\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.945837\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.794783\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.953828\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.812638\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.850912\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.705638\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.760828\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.654788\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.771188\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 1.799592\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.547697\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 1.630894\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.728762\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.668738\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.574558\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.709838\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.480381\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 1.605322\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.747478\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.499688\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.338468\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.431206\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.361022\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.520517\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.510579\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 1.514124\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.534731\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.537950\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.705619\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 1.559221\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.381372\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.223239\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.576696\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.333349\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.695803\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.398272\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.633492\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.161300\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.247193\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.446088\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.620902\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.358147\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.522268\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.262567\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.367360\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.471023\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.460485\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.482912\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.467591\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.236208\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.359602\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.215333\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.156693\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.465633\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.957648\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.186227\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.167020\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.226835\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.424780\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.335728\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.374166\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.347278\n",
      "\n",
      "Test set: Average loss: 0.7356, Accuracy: 8662/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.329179\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.422379\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.125826\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 1.468862\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.117106\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.334885\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.311750\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.240460\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.061759\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.350260\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.207782\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.249143\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.187815\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 1.206516\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 1.311816\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 1.206501\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.295768\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 1.095687\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 1.128488\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 1.166307\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.132873\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 1.254857\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 1.400715\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 1.184690\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 1.084875\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.253053\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 1.265523\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 1.076290\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 1.207842\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 1.046774\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.179610\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 1.308242\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.983048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-358e03f38b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mtraining_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-358e03f38b0c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, training_config)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5b875ac2da66>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 254\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     50\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     51\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch, training_config):\n",
    "  \"\"\"Train network model\n",
    "    Args:\n",
    "      epoch - current epoch\n",
    "      training_config - training configuration tuple\n",
    "  \"\"\"\n",
    "    \n",
    "  (train_loader, model, optimizer, flags) = training_config\n",
    "  model.train()\n",
    "  for (batch_idx, (data, target)) in enumerate(train_loader):\n",
    "    if flags.cuda:\n",
    "        (data, target) = (data.cuda(), target.cusa())\n",
    "    (data, target) = (Variable(data), Variable(target))\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % flags.log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "  torch.save(model.state_dict(), flags.weights)\n",
    "  \n",
    "\n",
    "def test(test_loader, model, flags):\n",
    "  \"\"\"Test network\n",
    "    test_loader - test data loader\n",
    "    model - network model\n",
    "    flags - configuration parameters\n",
    "  \"\"\"\n",
    "    \n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  for (data, target) in test_loader:\n",
    "    if flags.cuda:\n",
    "        (data, target) = (data.cuda(), target.cusa())\n",
    "    (data, target) = (Variable(data, volatile=True), Variable(target))\n",
    "    output = model(data)\n",
    "    test_loss += F.nll_loss(output, target, size_average=False).data[0]  # sum up batch loss\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "model = LeNet()\n",
    "\n",
    "if flags.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "print('End=attachment')\n",
    "optimizer = optim.SGD(model.parameters(), lr=flags.lr, momentum=flags.momentum)\n",
    "training_config = (train_loader, model, optimizer, flags)\n",
    "for epoch in range(1, flags.epochs + 1):\n",
    "    train(epoch, training_config)\n",
    "    test(test_loader, model, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "widgets": {
   "state": {
    "3b860209fd524db79874d730ef475323": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "769b91cccae94fce80655dfdac528be9": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "865355fe8fa7423f8f71bf84792d1057": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "9fab060726564a0abe4273ed1df30182": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "affe91b7a70c4ae29c383a2cc1cd15c6": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
