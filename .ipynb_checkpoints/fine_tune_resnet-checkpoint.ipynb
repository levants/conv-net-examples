{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running training (fine tuning) and interface for ResNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure training flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from cnn.transfer.cnn_files import files as _files\n",
    "from cnn.transfer import dataset_config as datasets\n",
    "from cnn.transfer.data_visualizer import visualize_model\n",
    "from os.path import expanduser\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def join_home(dir_path):\n",
    "  \"\"\"Joins passed direcotry to home directory\n",
    "    Args:\n",
    "      dir_path - directory path\n",
    "    Returns:\n",
    "      path joined to home directory\n",
    "  \"\"\"\n",
    "  home_dir = expanduser('~')\n",
    "  full_dir = _files.join_path(home_dir, dir_path)\n",
    "  print(full_dir)\n",
    " \n",
    "  return full_dir\n",
    "\n",
    "def init_training_flags():\n",
    "    \n",
    "    tr_flags = type('TrainingFlags', (object,), {})\n",
    "    \n",
    "    tr_flags.fine_tune = True # Flag to choose between transfer and fine-tune model\n",
    "    # Configure training flags\n",
    "    tr_flags.fc_size =1024 # Fully connected layer size\n",
    "    \n",
    "    # Flags for convolutional layers\n",
    "    tr_flags.optimizer = 'adam'\n",
    "    tr_flags.batch_size = 32 # Training batch size\n",
    "    tr_flags.epochs = 5 # Number of training epochs\n",
    "    tr_flags.decay = 1e-6# Training decay\n",
    "    tr_flags.keep_prob = 0.2 # Dropout keep probability\n",
    "    tr_flags.keep_dense_prob = 0.5 # Dropout keep probability for fully connected layer\n",
    "    tr_flags.learning_rate = 0.0001 # Learning rate\n",
    "    tr_flags.weight_decay = 1e-4 # Weight decay\n",
    "    tr_flags.momentum = 0.9 # Learning momentum\n",
    "    tr_flags.val_precentage = 0.2 # Percentage of validation data\n",
    "    tr_flags.test_precentage = 0.2 # Percentage of test data\n",
    "    tr_flags.num_classes = 2 # Number of classes\n",
    "    tr_flags.trainable_layers = 15 # Number of layers to freeze\n",
    "\n",
    "    tr_flags.save_model = False # Prints data set file names and labels\n",
    "    tr_flags.image_width = 224 # Training image width\n",
    "    tr_flags.image_height = 224 # Training image height\n",
    "    tr_flags.image_channels = 3 # Training image height\n",
    "    tr_flags.verbose = True #'Log steps\n",
    "    tr_flags.plot = True #Flag to plot recognition\n",
    "    \n",
    "    # System configuration\n",
    "    tr_flags.num_workers=8\n",
    "\n",
    "    return tr_flags\n",
    "\n",
    "\n",
    "def get_cats_dogs_training_flags():\n",
    "\n",
    "    tr_flags = init_training_flags()\n",
    "    # Configure training flags\n",
    "    tr_flags.model_dir = join_home('models/cats_dogs/')\n",
    "    tr_flags.model = join_home('models/cats_dogs/resnet18.pth.tar') # Path to network model for prediction\n",
    "    tr_flags.weights = join_home('models/cats_dogs/resnet18.pth.tar') # Where to save the trained data\n",
    "\n",
    "    tr_flags.dataset_dir = join_home('datasets/cats_dogs/dataset_files/') #'Path to folders of labeled images for pre-processing and training.'\n",
    "    tr_flags.train_dir = join_home('datasets/cats_dogs/training') # Path to folders of labeled images for training.\n",
    "    tr_flags.val_dir = join_home('datasets/cats_dogs/validation') # Path to folders of labeled images for validation.\n",
    "    tr_flags.test_dir = join_home('datasets/cats_dogs/test') # Path to folders of labeled images for validation.\n",
    "\n",
    "    return tr_flags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Levan/models/cats_dogs/\n",
      "/Users/Levan/models/cats_dogs/resnet18.pth.tar\n",
      "/Users/Levan/models/cats_dogs/resnet18.pth.tar\n",
      "/Users/Levan/datasets/cats_dogs/dataset_files/\n",
      "/Users/Levan/datasets/cats_dogs/training\n",
      "/Users/Levan/datasets/cats_dogs/validation\n",
      "/Users/Levan/datasets/cats_dogs/test\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Levan/datasets/cats_dogs/training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6de535e16b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtr_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cats_dogs_training_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetune_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class_names - '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class_to_idx - '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Levan/git/conv-net-examples/cnn/transfer/finetune_model.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(flags)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;31m# Data augmentation and normalization for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;31m# Just normalization for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;34m(\u001b[0m\u001b[0mdataloders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Levan/git/conv-net-examples/cnn/transfer/dataset_config.py\u001b[0m in \u001b[0;36minit_datasets\u001b[0;34m(flags)\u001b[0m\n\u001b[1;32m     88\u001b[0m                                          transforms.Normalize([0.485, 0.456, 0.406],\n\u001b[1;32m     89\u001b[0m                                                               [0.229, 0.224, 0.225])])\n\u001b[0;32m---> 90\u001b[0;31m   image_datasets = {TRAIN_PHASE: datasets.ImageFolder(flags.train_dir, train_transforms),\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mVAL_PHASE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_trandorms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     TEST_PHASE: datasets.ImageFolder(flags.test_dir, validation_trandorms)}\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m     91\u001b[0m     def __init__(self, root, transform=None, target_transform=None,\n\u001b[1;32m     92\u001b[0m                  loader=default_loader):\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/Users/Levan/datasets/cats_dogs/training'"
     ]
    }
   ],
   "source": [
    "from cnn.transfer import finetune_model\n",
    "\n",
    "tr_flags = get_cats_dogs_training_flags()\n",
    "(class_names, class_to_idx) = finetune_model.run_training(tr_flags)\n",
    "print('class_names - ', class_names)\n",
    "print('class_to_idx - ', class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure interface flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_argparse = type('ArgumentParserImpl', (object,), {})\n",
    "in_argparse.print_help = lambda:print('No image or URL are given')\n",
    "\n",
    "def init_interface_flags():\n",
    "    \n",
    "    in_flags = type('InterfaceFlags', (object,), {})\n",
    "    # Configure interface flags\n",
    "    in_flags.fc_size =1024 # Fully connected layer size\n",
    "    in_flags.num_classes = 2 # Number of classes\n",
    "    in_flags.save_model = False #Prints data set file names and labels\n",
    "    in_flags.image_width = 224 # Training image width\n",
    "    in_flags.image_height = 224 # Training image height\n",
    "    in_flags.image_channels = 3 # Training image height\n",
    "    in_flags.verbose = True #'Log steps\n",
    "    in_flags.labels = None\n",
    "    in_flags.plot = True #Flag to plot recognition\n",
    "    \n",
    "    # Network architecture\n",
    "    in_flags.keep_prob = 0.2 # Dropout keep probability\n",
    "    in_flags.keep_dense_prob = 0.5 # Dropout keep probability for fully connected layer\n",
    "\n",
    "    return in_flags\n",
    "\n",
    "\n",
    "def get_cats_dogs_interface_flags():\n",
    "\n",
    "    in_flags = init_interface_flags()\n",
    "    # Configure interface flags\n",
    "    in_flags.image = join_home('datasets/cats_dogs/test1/10336.jpg') # Path to image for prediction\n",
    "    in_flags.url = None # URL to image for prediction\n",
    "    in_flags.model=join_home('models/cats_dogs/resnet18.pth.tar') # Path to network model for prediction\n",
    "    in_flags.weights = join_home('models/cats_dogs/resnet50.h5') # Where to save the trained data\n",
    "    in_flags.train_dir = join_home('datasets/cats_dogs/resnet18.pth.tar') # Path to folders of labeled images for training.\n",
    "\n",
    "    return in_flags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs prediction on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pulsarai/datasets/revenue/dataset/test1/10336.jpg\n",
      "/home/pulsarai/models/revenue/resnet18.pth.tar\n",
      "/home/pulsarai/models/revenue/resnet18.pth.tar\n",
      "/home/pulsarai/datasets/revenue/dataset/training_files/training\n",
      "['apples', 'background', 'bags', 'bottles', 'firearms', 'glasses', 'knives', 'water_glasses', 'wine_glasses', 'wristwatch']\n",
      "{'bags': 2, 'bottles': 3, 'water_glasses': 7, 'firearms': 4, 'wristwatch': 9, 'wine_glasses': 8, 'knives': 6, 'apples': 0, 'background': 1, 'glasses': 5}\n",
      "/home/pulsarai/datasets/revenue/dataset/test1\n",
      "Variable containing:\n",
      "-0.4159 -0.9739  0.5404  0.5119 -0.0790 -0.8078  0.3341  0.6180  0.2277  0.6612\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "[9]\n",
      "predicted: wristwatch\n",
      "/home/pulsarai/datasets/revenue/dataset/test1\n",
      "Variable containing:\n",
      "-0.3564  0.2263  0.4608  0.4497 -0.3473 -0.3092 -0.2661  0.3706  0.3678  0.1494\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "[2]\n",
      "predicted: bags\n",
      "/home/pulsarai/datasets/revenue/dataset/test1\n",
      "Variable containing:\n",
      "-0.6384 -0.4509 -0.3255  0.2486 -0.2523  0.2014 -0.4065  0.4247  1.6346  0.3829\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "[8]\n",
      "predicted: wine_glasses\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from cnn.transfer import (network_model as networks,\n",
    "                          network_interface as interface)\n",
    "from cnn.resnet.resnet import resnet18\n",
    "from utils.config import dataset_reader as reader\n",
    "\n",
    "text = widgets.Text()\n",
    "button = widgets.Button(description='Submit')\n",
    "display(text)\n",
    "display(button)\n",
    "\n",
    "image_suffix = ''\n",
    "\n",
    "in_flags = get_revenue_interface_flags()\n",
    "model = networks.init_model_and_weights(in_flags, resnet18)\n",
    "model.eval()\n",
    "(class_names, index_labels) = datasets.read_labels(in_flags.train_dir)\n",
    "print(class_names)\n",
    "print(index_labels)\n",
    "\n",
    "def retrieve_labels(flags):\n",
    "  \"\"\"Retrieve label names\n",
    "    Args:\n",
    "      flags - training flags\n",
    "    Returns:\n",
    "      training labels\n",
    "  \"\"\"\n",
    "  return datasets.read_labels(flags.train_dir)\n",
    "\n",
    "\n",
    "from cnn.transfer.dataset_config import validation_trandorms as transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def handle_text(sender_value):\n",
    "    image_suffix = text.value\n",
    "    image_prefix = join_home('datasets/revenue/dataset/test1')\n",
    "    image_path = _files.join_path(image_prefix, image_suffix)\n",
    "    \n",
    "    \n",
    "    predictions = interface.run_on_path(model, image_path)\n",
    "    print(predictions)\n",
    "    (_, preds) = torch.max(predictions.data, 1)\n",
    "    data_idx = Variable(preds).data.numpy()\n",
    "    print(data_idx)\n",
    "    print('predicted: {}'.format(class_names[data_idx[0]]))\n",
    "    \n",
    "button.on_click(handle_text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "widgets": {
   "state": {
    "3b860209fd524db79874d730ef475323": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "769b91cccae94fce80655dfdac528be9": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "865355fe8fa7423f8f71bf84792d1057": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "9fab060726564a0abe4273ed1df30182": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "affe91b7a70c4ae29c383a2cc1cd15c6": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
